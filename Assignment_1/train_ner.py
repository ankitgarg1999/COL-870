# -*- coding: utf-8 -*-
"""train_ner.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1npPgdVpD6i7f87T6LpTNpFmizexVHksL
"""

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence
import torch.optim as optim
import torch.nn.functional as F
from torch.nn.utils import clip_grad_norm_
import math
import sys, getopt
import random

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


######### Supporting Functions ##############
def load_glove(filename):
  filef = open(filename, 'r')
  raw_data = filef.readlines()

  embeddings = {}

  for data in raw_data:
    emb_list = data.split(' ')
    emb_list[100] = emb_list[100][:len(emb_list)-1]
    word = emb_list[0]
    emb_float = torch.tensor([float(x) for x in emb_list[1:]], dtype=torch.float32)
    embeddings[word] = emb_float

  return embeddings

def random_word_vec():

  embedding = torch.tensor([random.gauss(0,0.1) for x in range (0,100)])
  return embedding

def get_glove_embeddings(glove, stoi_ftr_pt):

  embeddings = []

  for word in stoi_ftr_pt.keys():
    
    if word in glove.keys():
      embeddings.append(glove[word])

    else:
      embeddings.append(random_word_vec())

  embeddings_t = torch.stack([x for x in embeddings])
  return embeddings_t

############ Data Read ################

# Reads the file, extract sentences as a dictionary and append to a list 

def data_read(filename): 

  file = open(filename, 'r')
  raw_data = file.readlines()

  dataset = []
  prev = 0

  for i in range (1,len(raw_data)):

    if (raw_data[i][0] == '\n'):

      data_list = raw_data[prev+1:i]
      data_dict = {}

      sen = []
      pos = []
      act = []
      ent = []

      for j in range(0,len(data_list)):
        temp = data_list[j].split(" ")
        sen.append(temp[0])
        pos.append(temp[1])
        act.append(temp[2])
        ent.append(temp[3][:-1])

      data_dict['sen'] = sen 
      data_dict['pos'] = pos
      data_dict['act'] = act 
      data_dict['ent'] = ent

      dataset.append(data_dict)

      prev = i

  return dataset

# Take the sentences and start indexing every new word ecountered 
# ind 0 is for PAD and 1 is for UNK 

def build_vocab (dataset):
  stoi_ftr = {}
  stoi_ent = {}
  stoi_char= {}

  stoi_ftr['<PAD>']  = 0
  stoi_ftr['<UNK>']  = 1
  stoi_char['<PAD>'] = 0 
  stoi_char['<UNK>'] = 1

  for sentence in dataset:
    for word in sentence['sen']:
      if word not in stoi_ftr.keys():
        ind = len(stoi_ftr)
        stoi_ftr[word] = ind
      for char in word:
        if char not in stoi_char.keys():
          ind = len(stoi_char)
          stoi_char[char] = ind
    
    for ent in sentence['ent']:
      if ent not in stoi_ent.keys():
        ind = len(stoi_ent)
        stoi_ent[ent] = ind

  return stoi_ftr, stoi_ent, stoi_char

def build_vocab_pt (dataset1, dataset2, dataset3):
  stoi_ftr = {}
  stoi_char= {}
  stoi_ftr['<PAD>'] = 0
  stoi_char['<PAD>']= 0
  stoi_ftr['<UNK>'] = 1
  stoi_char['<UNK>']= 1

  for sentence in dataset1:
    for word in sentence['sen']:
      if word not in stoi_ftr.keys():
        ind = len(stoi_ftr)
        stoi_ftr[word] = ind
      for char in word:
        if char not in stoi_char.keys():
          ind = len(stoi_char)
          stoi_char[char] = ind

  for sentence in dataset2:
    for word in sentence['sen']:
      if word not in stoi_ftr.keys():
        ind = len(stoi_ftr)
        stoi_ftr[word] = ind
      for char in word:
        if char not in stoi_char.keys():
          ind = len(stoi_char)
          stoi_char[char] = ind

  for sentence in dataset3:
    for word in sentence['sen']:
      if word not in stoi_ftr.keys():
        ind = len(stoi_ftr)
        stoi_ftr[word] = ind
      for char in word:
        if char not in stoi_char.keys():
          ind = len(stoi_char)
          stoi_char[char] = ind

  return stoi_ftr , stoi_char

# take the dataset i.e. list of dictionaries and convert each dictionary/sentence to a list with 
# token replaced by vocab index 

def stoi_data(dataset, stoi_ftr, stoi_ent, stoi_char):
  X   = []
  Y   = []
  XC  = []

  for data in dataset:
    sentence = []
    sent_c = []

    for word in data['sen']:
      word_c = []
      if word in stoi_ftr.keys():
        sentence.append(stoi_ftr[word])
      else:
        sentence.append(stoi_ftr['<UNK>'])

      for char in word: 
        if char in stoi_char.keys():
          word_c.append(stoi_char[char])
        else:
          word_c.append(stoi_char['<UNK>'])
      sent_c.append(word_c)

    labels = []
    for ent in data['ent']:
      labels.append(stoi_ent[ent])

    X.append(sentence)
    Y.append(labels)
    XC.append(sent_c)

  return X,Y,XC

class MyDataset (Dataset):

  def __init__(self, X, Y, XC):
    self.X = X 
    self.Y = Y
    self.XC = XC

  def __len__(self):
    return len(self.X)

  def __getitem__(self, idx):
    return torch.tensor(self.X[idx]), torch.tensor(self.Y[idx]) , self.XC[idx]

# diff sentence have different word/ tocken take max and pad for each batch 
def MyCollate(batch):
  ftr, ent, char = zip(*batch)
  ftr_len = [x.shape[0] for x in ftr]
  ent_len = [x.shape[0] for x in ent]
  ftr = nn.utils.rnn.pad_sequence(ftr, batch_first=True, padding_value=0)
  ent = nn.utils.rnn.pad_sequence(ent, batch_first=True, padding_value=-1)
  # char= nn.utils.rnn.pad_sequence(char, batch_first=True, padding_value=0)
  max_cols = max([len(row) for batch in char for row in batch])
  max_rows = max([len(batch) for batch in char])
  padded = [batch + [[0] * (max_cols)] * (max_rows - len(batch)) for batch in char]
  padded = torch.tensor([row + [0] * (max_cols - len(row)) for batch in padded for row in batch])
  padded = padded.view(-1, max_rows, max_cols)
  return ftr, ent, padded, ftr_len, ent_len

def get_data(data_path, use_glove):

  train_data  = data_read(data_path + '/train.txt')
  val_data    = data_read(data_path + '/dev.txt')
  test_data   = data_read(data_path + '/test.txt')

  stoi_ftr, stoi_ent, stoi_char = build_vocab(train_data)
  stoi_ftr_pt, stoi_char_pt = build_vocab_pt(train_data, val_data, test_data)
  
  # Convert text to index
  if (use_glove):
    X_train, Y_train, XC_train = stoi_data(train_data, stoi_ftr_pt, stoi_ent, stoi_char_pt)
    X_val, Y_val, XC_val = stoi_data(val_data, stoi_ftr_pt, stoi_ent, stoi_char_pt)
    X_test, Y_test, XC_test = stoi_data(test_data, stoi_ftr_pt, stoi_ent, stoi_char_pt)
  else:
    X_train, Y_train, XC_train = stoi_data(train_data, stoi_ftr, stoi_ent, stoi_char)
    X_val, Y_val, XC_val = stoi_data(val_data, stoi_ftr, stoi_ent, stoi_char)
    X_test, Y_test, XC_test = stoi_data(test_data, stoi_ftr, stoi_ent, stoi_char)

  train_dataset = MyDataset(X_train,Y_train, XC_train)
  val_dataset = MyDataset(X_val,Y_val, XC_val)
  test_dataset = MyDataset(X_test,Y_test, XC_test)

  b_size = 128
  # Data loader for train, val and test data

  train_loader = DataLoader(train_dataset, batch_size = b_size, shuffle=True, collate_fn=MyCollate)
  val_loader = DataLoader(val_dataset, batch_size = b_size, shuffle=False, collate_fn=MyCollate)
  test_loader = DataLoader(test_dataset, batch_size = b_size, shuffle=False, collate_fn=MyCollate)

  vocab_size = None
  char_size = None
  if (use_glove):
    vocab_size = len(stoi_ftr_pt.keys())
    char_size = len(stoi_char_pt.keys())
  else:
    vocab_size = len(stoi_ftr.keys())
    char_size = len(stoi_char.keys())

  return train_loader, val_loader , vocab_size, char_size, stoi_ftr, stoi_ent, stoi_char, stoi_ftr_pt, stoi_char_pt

################################################################

## Models

#### Implementation of lstm with layer normalization

class MyLstmCell (nn.Module):

  def __init__ (self, input_size, hidden_size):

    super(MyLstmCell, self).__init__()

    self.hidden_size = hidden_size 
    self.input_size = input_size 

    self.weight_ii = nn.Parameter(((torch.rand([hidden_size, input_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)
    self.bias_ii = nn.Parameter(((torch.rand([hidden_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)
    self.weight_if = nn.Parameter(((torch.rand([hidden_size, input_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)
    self.bias_if = nn.Parameter(((torch.rand([hidden_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)
    self.weight_ig = nn.Parameter(((torch.rand([hidden_size, input_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)
    self.bias_ig = nn.Parameter(((torch.rand([hidden_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)
    self.weight_io = nn.Parameter(((torch.rand([hidden_size, input_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)
    self.bias_io = nn.Parameter(((torch.rand([hidden_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)

    self.weight_hi = nn.Parameter(((torch.rand([hidden_size, hidden_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)
    self.bias_hi = nn.Parameter(((torch.rand([hidden_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)
    self.weight_hf = nn.Parameter(((torch.rand([hidden_size, hidden_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)
    self.bias_hf = nn.Parameter(((torch.rand([hidden_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)
    self.weight_hg = nn.Parameter(((torch.rand([hidden_size, hidden_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)
    self.bias_hg = nn.Parameter(((torch.rand([hidden_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)
    self.weight_ho = nn.Parameter(((torch.rand([hidden_size, hidden_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)
    self.bias_ho = nn.Parameter(((torch.rand([hidden_size], dtype=torch.float))*2 - 1)/math.sqrt(hidden_size), requires_grad = True)

  def forward (self, x, h_o , c_o , ln):

    b_size = x.shape[0]

    h_o = h_o.cuda()
    c_o = c_o.cuda()

    i = torch.sigmoid(torch.matmul(x, torch.transpose(self.weight_ii, 0, 1)) + self.bias_ii 
                      + torch.matmul(h_o, torch.transpose(self.weight_hi, 0, 1)) + self.bias_hi)

    f = torch.sigmoid(torch.matmul(x, torch.transpose(self.weight_if, 0, 1)) + self.bias_if
                      + torch.matmul(h_o, torch.transpose(self.weight_hf, 0, 1)) + self.bias_hf)

    g = torch.tanh(torch.matmul(x, torch.transpose(self.weight_ig, 0, 1)) + self.bias_ig 
                      + torch.matmul(h_o, torch.transpose(self.weight_hg, 0, 1)) + self.bias_hg)

    o = torch.sigmoid(torch.matmul(x, torch.transpose(self.weight_io, 0, 1)) + self.bias_io 
                      + torch.matmul(h_o, torch.transpose(self.weight_ho, 0, 1)) + self.bias_ho)

    c = f * c_o + i * g

    h = o * torch.tanh( ln(c) )

    return h, c

class MyLstm (nn.Module):

  def __init__ (self, input_size, hidden_size):

    super(MyLstm, self).__init__()

    self.input_size = input_size
    self.hidden_size = hidden_size

    self.lstm_f = MyLstmCell(input_size, hidden_size)
    self.lstm_b = MyLstmCell(input_size, hidden_size)

    self.ln = nn.LayerNorm([hidden_size])

  def forward(self, x):

    b_size = x.shape[0]
    T = x.shape[1]

    h_f = [torch.zeros(b_size, self.hidden_size)]
    c_f = [torch.zeros(b_size, self.hidden_size)]

    h_b = [torch.zeros(b_size, self.hidden_size)]
    c_b = [torch.zeros(b_size, self.hidden_size)]

    for t in range(0,T):

      h_f_t, c_f_t = self.lstm_f(x[:, t, :], h_f[-1], c_f[-1], self.ln)
      h_b_t, c_b_t = self.lstm_b(x[:, T-1-t, :], h_b[-1], c_b[-1], self.ln)

      h_f_t = h_f_t.cuda()
      c_f_t = c_f_t.cuda()
      h_b_t = h_b_t.cuda()
      c_b_t = c_b_t.cuda()

      h_f.append(h_f_t)
      c_f.append(h_f_t)

      h_b.append(h_b_t)
      c_b.append(c_b_t)

    h_f = h_f[1:]
    h_b = h_b[1:]

    h_b.reverse()

    output = torch.stack([torch.cat((h_f[i], h_b[i]), dim = 1) for i in range (0,T)], dim = 1)
    return output

class model(nn.Module):

  def __init__(self, vocab_size, char_size, hidden_size, Glove, pre_trained, layer_norm, char_emb ,char_hidden_size = 25):

    super(model, self).__init__()

    self.embedding = None
    self.layer_norm = layer_norm
    self.char_hidden_size = char_hidden_size
    self.char_emb = char_emb


    self.char_embedding = None 
    self.char_lstm = None 

    if (not char_emb):
      self.char_hidden_size = 0 
    else:
      self.char_embedding = nn.Embedding(char_size, char_hidden_size, 0)
      self.char_lstm      = nn.LSTM(input_size = char_hidden_size, hidden_size = char_hidden_size, batch_first = True,bidirectional = True)

    if (pre_trained):
      self.embedding = nn.Embedding.from_pretrained(embeddings = Glove, freeze = False, padding_idx = 0)
    else:
      self.embedding = nn.Embedding(vocab_size, 100, 0)

    self.lstm = None

    if (layer_norm):
      self.lstm = MyLstm(input_size = (100+2*self.char_hidden_size), hidden_size = hidden_size) 

    else:
      self.lstm = nn.LSTM(input_size = (100 + 2*self.char_hidden_size) , hidden_size = hidden_size, batch_first = True,bidirectional = True) 
    
    self.linear = nn.Linear((2*hidden_size),17)

  def forward(self, x, xc):

    x = self.embedding(x)
    if (self.char_emb):
      xc = self.char_embedding(xc)
      shape = xc.shape
      xc = xc.view(-1, shape[2] ,self.char_hidden_size)
      xc = self.char_lstm(xc)

      xc = xc[0].view(shape[0],shape[1],shape[2],-1)
      x = torch.cat((x,xc[:,:,0,self.char_hidden_size:], xc[:,:,-1,0:self.char_hidden_size]), 2)

    x = self.lstm(x)
    y = None

    if (self.layer_norm):
      y  = self.linear(x)
    else:
      y = self.linear(x[0])

    return F.softmax(y, dim = 2)

##### CRF #######

class MyCRF(nn.Module):

  def __init__(self, num_labels = 17 ):

    super(MyCRF, self).__init__()

    self.num_labels = num_labels
    A_init = ((torch.rand([num_labels,num_labels],dtype=torch.float))*2 - 1)*0.1 
    self.A = nn.Parameter(A_init, requires_grad = True)
  
  def forward(self, x, y):
    """
    x is scores of sequence - shape [batch_size, sentence_length, num_labels]
    y are the gold labels of the sequence - shape [batch_size, sentence_length]
    Outputs the most likely label sequence
    """
    
    if (self.training):
      mls = self.most_likely_sequence(x, y)
      nll = self.loss(x, y)
      return mls, nll

    else:
      mls = self.most_likely_sequence(x, y)
      return mls

  def most_likely_sequence(self, x, y):
    """
    Viterbi decoding algorithm
    """

    with torch.no_grad():

      batch_pred = []
      batch_len, seq_len, num_labels = x.shape 
      mask = None

      if (self.training):
        mask = (y >= 0).float() # [batch_size, seq_len] 
      else:
        mask = torch.zeros([batch_len, seq_len])
        for i in range(0, batch_len):
          mask[i, :y[i]] = 1 

      lengths = torch.sum(mask, dim = 1)
      dp = torch.zeros([batch_len, seq_len, num_labels, 2])
      dp = dp - 1e5
      dp = dp.cuda()

      for j in range(0, num_labels):
        dp[:, 0, j, 0] = x[:, 0, j]

      for j in range(1, seq_len):
        for k in range(0, num_labels):
          for l in range(0, num_labels):
            temp = dp[:, j-1, k, 0] + self.A[k][l] + x[:, j, l]
            check = (temp > dp[:, j, l, 0]).float()
            dp[:, j, l, 0] = ((1 - check)*(dp[:, j, l, 0])) + check*temp
            dp[:, j, l, 1] = ((1 - check)*(dp[:, j, l, 1])) + check*k

      for i in range (0, batch_len):
        max_val = -1e6
        max_ind = -1

        length = int(lengths[i])

        for j in range(0, num_labels):
          if (dp[i][length-1][j][0] > max_val):
            max_val = dp[i][length-1][j][0]
            max_ind = j

        pred = []
        pred.append(max_ind)

        for j in range(length-1, 0, -1):
          prev = pred[-1]
          pred.append(int(dp[i][j][prev][1].item()))

        pred.reverse()
        batch_pred.append(pred)

      return batch_pred # A list of lists contataining the prediction

  def partition_function (self, x, y):
    """
    Softmax normalization constant Z
    """
  
    batch_len, seq_len, num_labels = x.shape 
    mask = (y >= 0).float() # [batch_size, seq_len] 
    lengths = torch.sum(mask, dim = 1)

    dp = torch.zeros([batch_len, seq_len, num_labels, num_labels]) # 3rd dimension stores the pre exponential stuff to take log later
    dp = dp.cuda()

    log_alpha = torch.zeros([batch_len, seq_len, num_labels]).cuda()

    for j in range(0, num_labels):#k+1'
      for k in range(0, num_labels):#k'
        dp[:, 0, j, k] =  x[:, 0, k] + self.A[k][j]

    log_alpha[:, 0, :] = torch.logsumexp(dp[:, 0,: , :].clone(), 2)

    for j in range(1, seq_len-1): # word i.e. k 
      for k in range(0, num_labels):# y(k'+1) 
        for l in range(0, num_labels):# yk' 

          dp[:, j, k, l] =  mask[:, j]*(x[:, j, l] + log_alpha[:, j-1, l].clone() + self.A[l][k]) + (1 - mask[:, j])*(dp[:, j-1, k, l])

      log_alpha[:, j, :] = torch.logsumexp(dp[:, j, :, :].clone(), 2)

    for l in range(0, num_labels): # inner index i.e. yk'           
      dp[:, -1, 0, l] =  x[:, -1, l] + log_alpha[:, -2, l].clone()

    z = torch.logsumexp(dp[:, -1, 0, :].clone(),1)
    return z

  def loss(self, x, y):
    """
    Calculates loss for the crf model
    This function called only during training time
    """

    batch_len, seq_len, num_labels = x.shape 
    Z = self.partition_function(x, y) ## [batch_size] vector
    
    sum = torch.zeros(batch_len)
    sum = sum.cuda()

    mask = (y >= 0).float() # [batch_size, seq_len] 

    for i in range(0, seq_len):

      x_t = x[:, i] # All the scores at the the i th time step, [batch_size, num_labels]
      p_y_t = x_t[range(batch_len),(y[:, i]).tolist()] # [batch_size] scores corresponding to the correct labels.
      sum = sum + mask[:, i]*p_y_t

      if (i > 0):
        sum = sum + mask[:, i]*(self.A[(y[:, i]).tolist(),(y[:, i-1]).tolist()])

    ll = (- sum + Z).mean()
    return ll

class crf_bilstm(nn.Module):

  def __init__(self, vocab_size, hidden_size):

    super(crf_bilstm, self).__init__()

    self.embedding = nn.Embedding(vocab_size, 100, 0)

    self.lstm = nn.LSTM(input_size = 100, hidden_size = hidden_size, batch_first = True,bidirectional = True) 
    self.linear = nn.Linear((2*hidden_size),17)
    self.crf = MyCRF(num_labels = 17)

  def forward(self, x, label = None):

    x = self.embedding(x)
    x = self.lstm(x)
    y = self.linear(x[0])
    return self.crf(y, label)
    
################################################################ 

##Train Model ############
def train(neural_net, optm, train_loader, val_loader, output_path):

  epochs = 500

  for i in range(0,epochs):

    loss_t = 0
    # print("Epoch No: "+ str(i+1))
    for data in train_loader:

      optm.zero_grad()

      input = data[0]
      label = data[1]
      input = input.cuda()
      label = label.cuda()
      char = data[2]
      char = char.cuda()    

      output = neural_net.forward(input, char)

      ll = 0

      label = label.view(-1)
      mask = (label >= 0)
      mask_f = mask.float()

      output = output.view(-1,17)
      ll = -torch.log(output[range(output.shape[0]), label.view(-1)])
      ll = (ll * mask_f).masked_select(mask).mean()
      ll.backward()

      clip_grad_norm_(neural_net.parameters(), max_norm=5, norm_type=2)
      
      optm.step()
      loss_t = loss_t + (ll*input.shape[0])

    loss_t = loss_t / 37206
    
    if ((i+1)%10 == 0):

      if ((i+1) == 10):
        prev = 100

      loss_v = 0
      with torch.no_grad():
        for data in val_loader:
          input = data[0]
          label = data[1]
          char = data[2]
          char = char.cuda()
          input = input.cuda()
          label = label.cuda()

          output = neural_net.forward(input, char)
          ll = 0

          label = label.view(-1)
          mask = (label >= 0)
          mask_f = mask.float()

          output = output.view(-1,17)
          ll = -torch.log(output[range(output.shape[0]), label.view(-1)])
          ll = (ll * mask_f).masked_select(mask).mean()
          loss_v = loss_v + (ll*input.shape[0])

      loss_v = loss_v / 12402
      print("Epoch: " + str(i+1) + " Training Loss = "+str(loss_t)+"; Val Loss = "+str(loss_v))

      if (loss_v < prev):
        torch.save(neural_net, output_path)
      if  (prev - loss_t < 0.01):
        print("Training Saturated at Epoch: "+ str(i+1))
        break 
      prev = loss_t       

def train_crf(neural_net, optm, train_loader, val_loader, output_path):

  epochs = 5
  
  print("CRF Training has started")

  for i in range(0,epochs):

    print("Epoch "+str(i+1)+" started:")

    for data in train_loader:

      optm.zero_grad()

      input = data[0]
      label = data[1]
      input = input.cuda()
      label = label.cuda()    

      mls, ll = neural_net.forward(input, label)
      ll.backward()

      clip_grad_norm_(neural_net.parameters(), max_norm=5, norm_type=2)
      
      optm.step()
      torch.cuda.empty_cache()   
    torch.save(neural_net, output_path)



################ Parse Command line arguments ##############

use_glove = False 
use_ce    = False 
use_ln    = False
use_crf   = False

data_path         = "" 
output_path       = ""
glove_file        = ""
vocab_file        = ""

argumentList = sys.argv[1:]
 
# Options
options = ""
 
# Long options
long_options = ["initialization=","char_embeddings=","layer_normalization=","crf=","data_dir=","output_file=", "glove_embeddings_file=",
                "vocabulary_output_file=" ]
 
try:
    # Parsing argument
    arguments, values = getopt.getopt(argumentList, options, long_options)
     
    # checking each argument
    for currentArgument, currentValue in arguments:
                  
      if currentArgument in ("--data_dir"):
        data_path = currentValue 

      elif currentArgument in ("--output_file"):
        output_path = currentValue

      elif currentArgument in ("--glove_embeddings_file"):
        glove_file = currentValue

      elif currentArgument in ("--vocabulary_output_file"):
        vocab_file = currentValue      
        
      elif currentArgument in ("--initialization"):
        if  ( currentValue == 'glove' ):
          use_glove = True 

      elif currentArgument in ("--char_embeddings"):
        if(currentValue == '1'):
          use_ce = True 
      
      elif currentArgument in ("--layer_normalization"):
        if(currentValue == '1'):
          use_ln = True 

      elif currentArgument in ("--crf"):
        if(currentValue == '1'):
          use_crf = True
             
except getopt.error as err:
    # output error, and return with an error code
    print (str(err))
  
print(data_path, output_path, glove_file, vocab_file, use_ce, use_glove, use_ln, use_crf)


if __name__ == "__main__":

  train_loader, val_loader, vocab_size, char_size, stoi_ftr, stoi_ent, stoi_char, stoi_ftr_pt, stoi_char_pt = get_data(data_path, use_glove)

  if (use_glove):
    stoi_ftu = stoi_ftr_pt
    stoi_ctu = stoi_char_pt
  else:
    stoi_ftu = stoi_ftr
    stoi_ctu = stoi_char

  with open(vocab_file, 'w') as f:
    f.write('\n')
    for word in stoi_ftu.keys():
      f.write(word+'\n')
    f.write('\n')
    for word in stoi_ctu.keys():
      f.write(word+'\n')
    f.write('\n')
    for word in stoi_ent.keys():
      f.write(word+'\n')
    f.write('\n')

  glove_embeddings = None 

  if (use_glove):
    glove             = load_glove(glove_file)
    glove_embeddings  = get_glove_embeddings(glove, stoi_ftr_pt)

  if (use_crf == False):
    neural_net = model(vocab_size, char_size ,100, glove_embeddings, pre_trained = use_glove, layer_norm = use_ln, char_emb = use_ce)

  else:
    neural_net = crf_bilstm(vocab_size, 100)

  neural_net = neural_net.cuda()
  optm = optim.SGD(neural_net.parameters(), lr = 0.01, momentum=0.9)

  if (use_crf):
    train_crf(neural_net, optm, train_loader, val_loader, output_path)
  else:
    train(neural_net, optm, train_loader, val_loader, output_path)