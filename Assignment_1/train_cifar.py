# -*- coding: utf-8 -*-
"""train_cifar_nt.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KpEsk7-R1tXrG3jtIIVAD6Vsgu7lXYRG
"""

import numpy    as np
import pandas   as pd
import torch
import torch.nn as nn
import pickle
import sys, getopt  

import torch.nn.functional  as F
import torch.optim          as optim
from torch.utils.data       import TensorDataset, DataLoader
# import matplotlib.pyplot as plt


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def normalize(X):
  X = X - np.mean(X, axis = 0)
  X = X / np.sqrt(np.var(X,axis = 0))

class E_Stopping(): 
  def __init__(self, patience=10): 
    self.patience = patience 
    self.prev = -1 
    self.count = 0 

  def check(self,val_loss): 
    if (self.prev == -1):
      self.prev = val_loss
      return 0
    elif (val_loss < self.prev):
      self.prev = val_loss
      self.count = 0 
      return 0
    elif (val_loss > self.prev):
      self.count = self.count + 1
      if (self.count > self.patience): 
        return 1 
      
    return 0

def get_data(data_path, b_size = 128): 

  ##scan the inputs 

  batch1, batch2, batch3, batch4, batch5, test = {},{},{},{},{},{}
  with open(data_path +'/data_batch_1', 'rb') as file_open:
    batch1 = pickle.load(file_open, encoding='bytes')
  with open(data_path +'/data_batch_2', 'rb') as file_open:
    batch2 = pickle.load(file_open, encoding='bytes')
  with open(data_path +'/data_batch_3', 'rb') as file_open:
    batch3 = pickle.load(file_open, encoding='bytes')
  with open(data_path +'/data_batch_4', 'rb') as file_open:
    batch4 = pickle.load(file_open, encoding='bytes')
  with open(data_path +'/data_batch_5', 'rb') as file_open:
    batch5 = pickle.load(file_open, encoding='bytes')
  with open(data_path +'/test_batch', 'rb') as file_open:
    test = pickle.load(file_open, encoding='bytes')

  ## Split into train/val 

  X_train = np.concatenate((batch1[b'data'],batch2[b'data'],batch3[b'data'],batch4[b'data']))
  Y_train = batch1[b'labels'] + batch2[b'labels'] + batch3[b'labels'] + batch4[b'labels']
  #Convert list to numpy
  Y_train = np.array(Y_train,dtype="float32")

  X_val = batch5[b'data']
  Y_val = batch5[b'labels']
  #Convert list to numpy
  Y_val = np.array(Y_val,dtype="float32")

  X_test = test[b'data']
  Y_test = test[b'labels']
  #Convert list to numpy
  Y_test = np.array(Y_test,dtype="float32")


  ## Normalise the inputs

  normalize(X_train)
  normalize(X_val)
  normalize(X_test)
  
  #Store as images 
  X_train = np.reshape(X_train,(40000,3,32,32))
  X_val   = np.reshape(X_val,(10000,3,32,32))
  X_test  = np.reshape(X_test,(10000,3,32,32))


  # Convert to tensors 
  X_train_t = torch.Tensor(X_train) # transform to torch tensor
  Y_train_t = torch.Tensor(Y_train)

  X_val_t = torch.Tensor(X_val) # transform to torch tensor
  Y_val_t = torch.Tensor(Y_val)

  X_test_t = torch.Tensor(X_test) # transform to torch tensor
  Y_test_t = torch.Tensor(Y_test)

  train_data   = TensorDataset(X_train_t,Y_train_t) # create your datset
  train_loader = DataLoader(train_data, 
                            batch_size=b_size, 
                            shuffle=True) # create your dataloader



  val_data   = TensorDataset(X_val_t,Y_val_t) # create your datset
  val_loader = DataLoader(val_data, 
                          batch_size=b_size, 
                          shuffle=True) # create your dataloader


  test_data   = TensorDataset(X_test_t,Y_test_t) # create your datset
  test_loader = DataLoader(test_data, 
                          batch_size=b_size, 
                          shuffle=True) # create your dataloader
  

  return train_loader,X_val_t, Y_val_t 

 
# Built In 
class torch_bn(nn.Module):

  def __init__(self,n,r):

    super(torch_bn, self).__init__()

    self.n = n
    self.r = r

    # 1st layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.b1 = nn.BatchNorm2d(16)

    self.downsample1 = nn.Conv2d(16, 32, 1, stride = 2)
    self.downsample2 = nn.Conv2d(32, 64, 1, stride = 2)
    self.bd1 = nn.BatchNorm2d(32)
    self.bd2 = nn.BatchNorm2d(64)

    self.layers = nn.ModuleList()
    self.bn_layers = nn.ModuleList()
    # 2n layers
    for i in range(0,2*n):
      self.layers.append(nn.Conv2d(16, 16, 3, padding=1))
      self.bn_layers.append(nn.BatchNorm2d(16))
 
    # 2n layers
    self.layers.append(nn.Conv2d(16, 32, 3, stride = 2, padding=1))
    self.bn_layers.append(nn.BatchNorm2d(32))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(32, 32, 3, padding=1))
      self.bn_layers.append(nn.BatchNorm2d(32))

    # 2n layers
    self.layers.append(nn.Conv2d(32, 64, 3, stride = 2, padding=1))
    self.bn_layers.append(nn.BatchNorm2d(64))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(64, 64, 3, padding=1))
      self.bn_layers.append(nn.BatchNorm2d(64))

    # last layer
    self.avg_pool = nn.AvgPool2d(8)
    
    #Output layer
    self.linear = nn.Linear(64,r)

  def forward(self, x):
    
    x = F.relu(self.b1(self.conv1(x)))
    
    prev = torch.zeros(1,1)

    for i in range (0,6*self.n):
      
      if (i%2 == 0):
        prev = x
        x = F.relu(self.bn_layers[i](self.layers[i](x)))
      elif (i == 2*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd1(self.downsample1(prev))))
      elif (i == 4*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd2(self.downsample2(prev))))
      else:
        x = F.relu(self.bn_layers[i](self.layers[i](x) + prev))

    x = self.avg_pool(x)
    x = x.view(-1,64)
    x = self.linear(x)

    return x


# No norm 

class no_norm(nn.Module):

  def __init__(self,n,r):

    super(no_norm, self).__init__()

    self.n = n
    self.r = r

    # 1st layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)

    self.downsample1 = nn.Conv2d(16, 32, 1, stride = 2)
    self.downsample2 = nn.Conv2d(32, 64, 1, stride = 2)

    self.layers = nn.ModuleList()
    # 2n layers
    for i in range(0,2*n):
      self.layers.append(nn.Conv2d(16, 16, 3, padding=1))
 
    # 2n layers
    self.layers.append(nn.Conv2d(16, 32, 3, stride = 2, padding=1))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(32, 32, 3, padding=1))

    # 2n layers
    self.layers.append(nn.Conv2d(32, 64, 3, stride = 2, padding=1))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(64, 64, 3, padding=1))

    # last layer
    self.avg_pool = nn.AvgPool2d(8)
    
    #Output layer
    self.linear = nn.Linear(64,r)

  def forward(self, x):
    
    x = F.relu(self.conv1(x))
    
    prev = torch.zeros(1,1)

    for i in range (0,6*self.n):
      
      if (i%2 == 0):
        prev = x
        x = F.relu(self.layers[i](x))
      elif (i == 2*self.n + 1):
        x = F.relu(self.layers[i](x) + self.downsample1(prev))
      elif (i == 4*self.n + 1):
        x = F.relu(self.layers[i](x) + self.downsample2(prev))
      else:
        x = F.relu(self.layers[i](x) + prev)

    x = self.avg_pool(x)
    x = x.view(-1,64)
    x = self.linear(x)

    return x

##### Batch Norm 

class bnorm(nn.Module):

  def __init__(self, h, w, d, decay = 0.1, epsilon = 1e-5):

    super(bnorm, self).__init__()

    self.gamma = nn.Parameter(torch.ones([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.beta = nn.Parameter(torch.zeros([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.epsilon = epsilon
    self.running_mean = 0
    self.running_cov = 0
    self.decay = decay

  def forward(self, x):

    if (self.training):
      mean = torch.reshape(torch.mean(x, (0,2,3)),(1, x.shape[1], 1, 1))
      self.running_mean = (1-self.decay)*(self.running_mean) + self.decay*(mean)
      cov = torch.reshape(torch.mean(torch.square(x - mean), (0,2,3)),(1, x.shape[1], 1, 1)) + self.epsilon
      self.running_cov = (1-self.decay)*(self.running_cov) + self.decay*(cov)
      x_n  = (x - mean) / torch.sqrt(cov)
      x_s  = (x_n * self.gamma) + self.beta
      return x_s

    else:
      x_n  = (x - self.running_mean) / torch.sqrt(self.running_cov)
      x_s  = (x_n * self.gamma) + self.beta
      return x_s

class batch_norm(nn.Module):

  def __init__(self,n,r=10):

    super(batch_norm, self).__init__()

    self.n = n
    self.r = r

    # 1st layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.b1 = bnorm(32, 32, 16)

    self.downsample1 = nn.Conv2d(16, 32, 1, stride = 2)
    self.downsample2 = nn.Conv2d(32, 64, 1, stride = 2)
    self.bd1 = bnorm(16, 16 ,32)
    self.bd2 = bnorm(8, 8, 64)

    self.layers = nn.ModuleList()
    self.bn_layers = nn.ModuleList()
    # 2n layers
    for i in range(0,2*n):
      self.layers.append(nn.Conv2d(16, 16, 3, padding=1))
      self.bn_layers.append(bnorm(32, 32, 16))
 
    # 2n layers
    self.layers.append(nn.Conv2d(16, 32, 3, stride = 2, padding=1))
    self.bn_layers.append(bnorm(16, 16, 32))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(32, 32, 3, padding=1))
      self.bn_layers.append(bnorm(16, 16, 32))

    # 2n layers
    self.layers.append(nn.Conv2d(32, 64, 3, stride = 2, padding=1))
    self.bn_layers.append(bnorm(8, 8, 64))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(64, 64, 3, padding=1))
      self.bn_layers.append(bnorm(8, 8, 64))

    # last layer
    self.avg_pool = nn.AvgPool2d(8)
    
    #Output layer
    self.linear = nn.Linear(64,r)

  def forward(self, x):
    
    x = F.relu(self.b1(self.conv1(x)))
    prev = torch.zeros((1,1))

    for i in range (0,6*self.n):
      
      if (i%2 == 0):
        prev = x
        x = F.relu(self.bn_layers[i](self.layers[i](x)))
      elif (i == 2*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd1(self.downsample1(prev))))
      elif (i == 4*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd2(self.downsample2(prev))))
      else:
        x = F.relu(self.bn_layers[i](self.layers[i](x) + prev))

    x = self.avg_pool(x)
    x = x.view(-1,64)
    x = self.linear(x)

    return x

### Layer NORM ###################

class lnorm(nn.Module):

  def __init__(self, h, w, d, decay = 0.1, epsilon = 1e-5):

    super(lnorm, self).__init__()

    self.gamma = nn.Parameter(torch.ones([1, d, h, w], dtype=torch.float), requires_grad = True)
    self.beta = nn.Parameter(torch.zeros([1, d, h, w], dtype=torch.float), requires_grad = True)
    self.epsilon = epsilon

  def forward(self, x):
    
    b_size = x.shape[0]
    mean = torch.reshape(torch.mean(x, (1,2,3)), (b_size,1,1,1))
    cov = torch.reshape(torch.mean(torch.square(x - mean), (1,2,3)), (b_size,1,1,1)) + self.epsilon 
    x_n  = (x - mean) / torch.sqrt(cov)
    x_s  = (x_n * self.gamma) + self.beta
    return x_s


class layer_norm(nn.Module):

  def __init__(self,n,r=10):

    super(layer_norm, self).__init__()

    self.n = n
    self.r = r

    # 1st layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.b1 = lnorm(32, 32, 16)

    self.downsample1 = nn.Conv2d(16, 32, 1, stride = 2)
    self.downsample2 = nn.Conv2d(32, 64, 1, stride = 2)
    self.bd1 = lnorm(16, 16 ,32)
    self.bd2 = lnorm(8, 8, 64)

    self.layers = nn.ModuleList()
    self.bn_layers = nn.ModuleList()
    # 2n layers
    for i in range(0,2*n):
      self.layers.append(nn.Conv2d(16, 16, 3, padding=1))
      self.bn_layers.append(lnorm(32, 32, 16))
 
    # 2n layers
    self.layers.append(nn.Conv2d(16, 32, 3, stride = 2, padding=1))
    self.bn_layers.append(lnorm(16, 16, 32))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(32, 32, 3, padding=1))
      self.bn_layers.append(lnorm(16, 16, 32))

    # 2n layers
    self.layers.append(nn.Conv2d(32, 64, 3, stride = 2, padding=1))
    self.bn_layers.append(lnorm(8, 8, 64))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(64, 64, 3, padding=1))
      self.bn_layers.append(lnorm(8, 8, 64))

    # last layer
    self.avg_pool = nn.AvgPool2d(8)
    
    #Output layer
    self.linear = nn.Linear(64,r)

  def forward(self, x):
    
    x = F.relu(self.b1(self.conv1(x)))
    prev = torch.zeros((1,1))

    for i in range (0,6*self.n):
      
      if (i%2 == 0):
        prev = x
        x = F.relu(self.bn_layers[i](self.layers[i](x)))
      elif (i == 2*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd1(self.downsample1(prev))))
      elif (i == 4*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd2(self.downsample2(prev))))
      else:
        x = F.relu(self.bn_layers[i](self.layers[i](x) + prev))

    x = self.avg_pool(x)
    x = x.view(-1,64)
    x = self.linear(x)

    return x

  
### Instance Norm ################

class inorm(nn.Module):

  def __init__(self, h, w, d, decay = 0.1, epsilon = 1e-5):

    super(inorm, self).__init__()

    self.gamma = nn.Parameter(torch.ones([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.beta = nn.Parameter(torch.zeros([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.epsilon = epsilon

  def forward(self, x):
    
    b_size = x.shape[0]
    channel = x.shape[1]
    mean = torch.reshape(torch.mean(x, (2,3)), (b_size, channel,1,1))
    cov = torch.reshape(torch.mean(torch.square(x - mean), (2,3)), (b_size,channel,1,1)) + self.epsilon 
    x_n  = (x - mean) / torch.sqrt(cov)
    x_s  = (x_n * self.gamma) + self.beta
    return x_s
  

class instance_norm(nn.Module):

  def __init__(self,n,r=10):

    super(instance_norm, self).__init__()

    self.n = n
    self.r = r

    # 1st layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.b1 = inorm(32, 32, 16)

    self.downsample1 = nn.Conv2d(16, 32, 1, stride = 2)
    self.downsample2 = nn.Conv2d(32, 64, 1, stride = 2)
    self.bd1 = inorm(16, 16 ,32)
    self.bd2 = inorm(8, 8, 64)

    self.layers = nn.ModuleList()
    self.bn_layers = nn.ModuleList()
    # 2n layers
    for i in range(0,2*n):
      self.layers.append(nn.Conv2d(16, 16, 3, padding=1))
      self.bn_layers.append(inorm(32, 32, 16))
 
    # 2n layers
    self.layers.append(nn.Conv2d(16, 32, 3, stride = 2, padding=1))
    self.bn_layers.append(inorm(16, 16, 32))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(32, 32, 3, padding=1))
      self.bn_layers.append(inorm(16, 16, 32))

    # 2n layers
    self.layers.append(nn.Conv2d(32, 64, 3, stride = 2, padding=1))
    self.bn_layers.append(inorm(8, 8, 64))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(64, 64, 3, padding=1))
      self.bn_layers.append(inorm(8, 8, 64))

    # last layer
    self.avg_pool = nn.AvgPool2d(8)
    
    #Output layer
    self.linear = nn.Linear(64,r)

  def forward(self, x):
    
    x = F.relu(self.b1(self.conv1(x)))
    prev = torch.zeros((1,1))

    for i in range (0,6*self.n):
      
      if (i%2 == 0):
        prev = x
        x = F.relu(self.bn_layers[i](self.layers[i](x)))
      elif (i == 2*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd1(self.downsample1(prev))))
      elif (i == 4*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd2(self.downsample2(prev))))
      else:
        x = F.relu(self.bn_layers[i](self.layers[i](x) + prev))

    x = self.avg_pool(x)
    x = x.view(-1,64)
    x = self.linear(x)

    return x

### Group Norm ################

class gnorm(nn.Module):

  def __init__(self, h, w, d, epsilon = 1e-5, group_size = 16):

    super(gnorm, self).__init__()

    self.gamma = nn.Parameter(torch.ones([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.beta = nn.Parameter(torch.zeros([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.epsilon = epsilon
    self.group_size = group_size

  def forward(self, x):
    
    b_size = x.shape[0]
    channel = x.shape[1]
    k = int(channel/self.group_size)

    x = torch.reshape(x, (b_size, self.group_size, k, x.shape[2], x.shape[3] ))

    mean = torch.reshape(torch.mean(x, (2,3,4)), (b_size, self.group_size, 1, 1, 1))
    cov = torch.reshape(torch.mean(torch.square(x - mean), (2, 3, 4)), (b_size,self.group_size, 1, 1, 1)) + self.epsilon 
    x_n  = (x - mean) / torch.sqrt(cov)
    x_n = torch.reshape(x_n, (b_size, channel, x.shape[3], x.shape[4]))
    x_s  = (x_n * self.gamma) + self.beta
    return x_s


class group_norm(nn.Module):

  def __init__(self,n,r=10):

    super(group_norm, self).__init__()

    self.n = n
    self.r = r

    # 1st layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.b1 = gnorm(32, 32, 16)

    self.downsample1 = nn.Conv2d(16, 32, 1, stride = 2)
    self.downsample2 = nn.Conv2d(32, 64, 1, stride = 2)
    self.bd1 = gnorm(16, 16 ,32)
    self.bd2 = gnorm(8, 8, 64)

    self.layers = nn.ModuleList()
    self.bn_layers = nn.ModuleList()
    # 2n layers
    for i in range(0,2*n):
      self.layers.append(nn.Conv2d(16, 16, 3, padding=1))
      self.bn_layers.append(gnorm(32, 32, 16))
 
    # 2n layers
    self.layers.append(nn.Conv2d(16, 32, 3, stride = 2, padding=1))
    self.bn_layers.append(gnorm(16, 16, 32))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(32, 32, 3, padding=1))
      self.bn_layers.append(gnorm(16, 16, 32))

    # 2n layers
    self.layers.append(nn.Conv2d(32, 64, 3, stride = 2, padding=1))
    self.bn_layers.append(gnorm(8, 8, 64))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(64, 64, 3, padding=1))
      self.bn_layers.append(gnorm(8, 8, 64))

    # last layer
    self.avg_pool = nn.AvgPool2d(8)
    
    #Output layer
    self.linear = nn.Linear(64,r)

  def forward(self, x):
    
    x = F.relu(self.b1(self.conv1(x)))
    prev = torch.zeros((1,1))

    for i in range (0,6*self.n):
      
      if (i%2 == 0):
        prev = x
        x = F.relu(self.bn_layers[i](self.layers[i](x)))
      elif (i == 2*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd1(self.downsample1(prev))))
      elif (i == 4*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd2(self.downsample2(prev))))
      else:
        x = F.relu(self.bn_layers[i](self.layers[i](x) + prev))

    x = self.avg_pool(x)
    x = x.view(-1,64)
    x = self.linear(x)

    return x



#### Batch Instance Norm ############

class binorm(nn.Module):

  def __init__(self, h, w, d, decay = 0.1, epsilon = 1e-5):

    super(binorm, self).__init__()

    self.gamma = nn.Parameter(torch.ones([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.beta = nn.Parameter(torch.zeros([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.rho = nn.Parameter(torch.zeros([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.epsilon = epsilon
    self.running_mean = 0
    self.running_cov = 0
    self.decay = decay

  def forward(self, x):

    b_size = x.shape[0]
    channel = x.shape[1]

    if (self.training):
      mean = torch.reshape(torch.mean(x, (0,2,3)),(1, x.shape[1], 1, 1))
      self.running_mean = (1-self.decay)*(self.running_mean) + self.decay*(mean)
      cov = torch.reshape(torch.mean(torch.square(x - mean), (0,2,3)),(1, x.shape[1], 1, 1)) + self.epsilon
      self.running_cov = (1-self.decay)*(self.running_cov) + self.decay*(cov)
      x_n1  = (x - mean) / torch.sqrt(cov)
      
      mean2 = torch.reshape(torch.mean(x, (2,3)), (b_size, channel,1,1))
      cov2 = torch.reshape(torch.mean(torch.square(x - mean2), (2,3)), (b_size,channel,1,1)) + self.epsilon 
      x_n2  = (x - mean2) / torch.sqrt(cov2)

      x_nc = x_n1*self.rho + x_n2*(1 - self.rho)
      x_s  = (x_nc * self.gamma) + self.beta
      return x_s


    else:
      x_n1  = (x - self.running_mean) / torch.sqrt(self.running_cov)
      
      mean2 = torch.reshape(torch.mean(x, (2,3)), (b_size, channel,1,1))
      cov2 = torch.reshape(torch.mean(torch.square(x - mean2), (2,3)), (b_size,channel,1,1)) + self.epsilon 
      x_n2  = (x - mean2) / torch.sqrt(cov2)

      x_nc = x_n1*self.rho + x_n2*(1 - self.rho)
      x_s  = (x_nc * self.gamma) + self.beta
      return x_s


class binstance_norm(nn.Module):

  def __init__(self,n,r=10):

    super(binstance_norm, self).__init__()

    self.n = n
    self.r = r

    # 1st layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.b1 = binorm(32, 32, 16)

    self.downsample1 = nn.Conv2d(16, 32, 1, stride = 2)
    self.downsample2 = nn.Conv2d(32, 64, 1, stride = 2)
    self.bd1 = binorm(16, 16 ,32)
    self.bd2 = binorm(8, 8, 64)

    self.layers = nn.ModuleList()
    self.bn_layers = nn.ModuleList()
    # 2n layers
    for i in range(0,2*n):
      self.layers.append(nn.Conv2d(16, 16, 3, padding=1))
      self.bn_layers.append(binorm(32, 32, 16))
 
    # 2n layers
    self.layers.append(nn.Conv2d(16, 32, 3, stride = 2, padding=1))
    self.bn_layers.append(binorm(16, 16, 32))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(32, 32, 3, padding=1))
      self.bn_layers.append(binorm(16, 16, 32))

    # 2n layers
    self.layers.append(nn.Conv2d(32, 64, 3, stride = 2, padding=1))
    self.bn_layers.append(binorm(8, 8, 64))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(64, 64, 3, padding=1))
      self.bn_layers.append(binorm(8, 8, 64))

    # last layer
    self.avg_pool = nn.AvgPool2d(8)
    
    #Output layer
    self.linear = nn.Linear(64,r)

  def forward(self, x):
    
    x = F.relu(self.b1(self.conv1(x)))
    prev = torch.zeros((1,1))

    for i in range (0,6*self.n):
      
      if (i%2 == 0):
        prev = x
        x = F.relu(self.bn_layers[i](self.layers[i](x)))
      elif (i == 2*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd1(self.downsample1(prev))))
      elif (i == 4*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd2(self.downsample2(prev))))
      else:
        x = F.relu(self.bn_layers[i](self.layers[i](x) + prev))

    x = self.avg_pool(x)
    x = x.view(-1,64)
    x = self.linear(x)

    return x

#########################
def get_val_loss (X, Y, model):

  total = 0
  correct = 0

  with torch.no_grad():
    ll = 0
    for i in range (0,X.shape[0],1000):
      
      labels = Y[i:i+1000]
      labels = labels.type(torch.LongTensor)
      labels = labels.cuda()

      pred = model.forward(X[i:i+1000,:,:,:])
      r = pred.size(0)
      pred = pred.data
      total = total + r
      max_val, max_ind = torch.max(pred, 1)
      com = (max_ind == labels)
      com = com.sum()
      correct = correct + com.item() 

      loss = nn.CrossEntropyLoss()
      ll = ll + loss(pred,labels)

  acc = (correct / total) * 100
  ll = ll*1000/X.shape[0]
  return acc , ll


########################

def train(neural_net, optm, scheduler, train_loader, X, Y, output_path, b_size=128):
  
  epochs    = 100
  patience  = 10

  e_stop = E_Stopping(patience)
  min_v_loss = 100

  early_stop = 1 
  flag = True 

  for i in range(0,epochs):

    loss_t = 0
    
    for data in train_loader:

      optm.zero_grad()

      input = data[0]
      label = data[1]
      label = label.type(torch.LongTensor)

      input = input.cuda()
      label = label.cuda()

      loss = nn.CrossEntropyLoss()
      output = neural_net.forward(input)
      ll = loss(output,label)
      ll.backward()
      optm.step()

      loss_t += (ll*b_size)

    scheduler.step()
    loss_t = loss_t / 40000
    
    if ((i+1)%10 == 0):
      print("Epoch: " + str(i+1) + " Loss = "+str(loss_t))

    _, val_loss = get_val_loss(X,Y,neural_net)

    if (val_loss <= min_v_loss):
      min_v_loss = val_loss
      torch.save(neural_net, output_path)

    if (flag and early_stop and e_stop.check(val_loss)):
      print("Early Stopping at Epoch: "+ str(i+1))
      break 
      flag = False
    




################ Parse Command line arguments ##############

normalisation = {
                 "bn":      0,
                 "in":      1, 
                 "bin":     2,
                 "ln":      3,
                 "gn":      4,
                 "nn":      5,
                 "torch_bn":6

}
model_to_use = [batch_norm, instance_norm, binstance_norm, layer_norm,group_norm, no_norm, torch_bn]
def optm_to_use(neural_net,n):
  choices = [
                optim.SGD(neural_net.parameters(), lr = 1, momentum=0.9, weight_decay= 0.0001), 
                optim.SGD(neural_net.parameters(), lr = 1, momentum=0.9, weight_decay= 0.0001), 
                optim.SGD(neural_net.parameters(), lr = 1, momentum=0.9, weight_decay= 0.0001), 
                optim.SGD(neural_net.parameters(), lr = 0.01, momentum=0.9, weight_decay= 0.0001),
                optim.SGD(neural_net.parameters(), lr = 0.1, momentum=0.9, weight_decay= 0.0001),
                optim.SGD(neural_net.parameters(), lr = 0.001, momentum=0.9, weight_decay= 0.0001),
                optim.SGD(neural_net.parameters(), lr = 0.1, momentum=0.9, weight_decay= 0.0001)
                ]
  return choices[n]

lmbda = lambda epoch: 0.96

def sched_to_use(optm,n):
  choices =  [
                optim.lr_scheduler.MultiplicativeLR(optm,lr_lambda=lmbda), 
                optim.lr_scheduler.MultiStepLR(optm, milestones=[50], gamma=0.1),
                optim.lr_scheduler.MultiStepLR(optm, milestones=[30], gamma=0.1),
                optim.lr_scheduler.MultiStepLR(optm, milestones=[30], gamma=1), 
                optim.lr_scheduler.MultiStepLR(optm, milestones=[20], gamma=0.1),
                optim.lr_scheduler.MultiStepLR(optm, milestones=[30], gamma=1),
                optim.lr_scheduler.MultiStepLR(optm, milestones=[30], gamma=1)]
  return choices[n]

norm_used = -1
data_path = ""
output_path = ""
numb_rcell = 0 

argumentList = sys.argv[1:]
 
# Options
options = "n:"
 
# Long options
long_options = ["data_dir=","n=", "normalization=","output_file="]
 
try:
    # Parsing argument
    arguments, values = getopt.getopt(argumentList, options, long_options)
     
    # checking each argument
    for currentArgument, currentValue in arguments:
                  
      if currentArgument in ("--data_dir"):
        data_path = currentValue 

      elif currentArgument in ("--output_file"):
        output_path = currentValue

      elif currentArgument in ("-n", "--n"):
        numb_rcell = currentValue

      elif currentArgument in ("--normalization"):
        norm_used = normalisation[currentValue]
             
except getopt.error as err:
    # output error, and return with an error code
    print (str(err))
print(data_path, output_path, numb_rcell, norm_used)
if __name__ == "__main__":

  train_loader, X_val_t, Y_val_t = get_data(data_path)
  X_val_t = X_val_t.to(device)
  Y_val_t = Y_val_t.to(device)
  neural_net = (model_to_use[norm_used])(int(numb_rcell), 10)
  neural_net = neural_net.cuda()
  optm = optm_to_use(neural_net, norm_used)
  scheduler = sched_to_use(optm, norm_used)

  train(neural_net, optm, scheduler, train_loader,X_val_t,Y_val_t, output_path)