# -*- coding: utf-8 -*-
"""test_cifar_nt.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Deam2Z0fTqKdthIAYF-trg8V0IbSwpHN
"""

import numpy    as np
import pandas   as pd
import torch
import torch.nn as nn
import pickle
import sys, getopt
import csv   

import torch.nn.functional  as F
import torch.optim          as optim
from torch.utils.data       import TensorDataset, DataLoader
# import matplotlib.pyplot as plt


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def normalize(X):
  X = X - np.mean(X, axis = 0)
  X = X / np.sqrt(np.var(X,axis = 0))


def get_data(data_path): 

  X_test = []
  with open(data_path, 'r') as csvfile:
    # creating a csv reader object
    csvreader = csv.reader(csvfile)
  
    for row in csvreader:
        X_test.append(row)

  X_test = np.array(X_test,dtype="float32")

  ## Normalise the inputs
  normalize(X_test)
  
  X_test  = np.reshape(X_test,(X_test.shape[0],3,32,32))

  X_test_t = torch.Tensor(X_test) # transform to torch tensor

  return X_test_t

 
# Built In 
class torch_bn(nn.Module):

  def __init__(self,n,r):

    super(torch_bn, self).__init__()

    self.n = n
    self.r = r

    # 1st layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.b1 = nn.BatchNorm2d(16)

    self.downsample1 = nn.Conv2d(16, 32, 1, stride = 2)
    self.downsample2 = nn.Conv2d(32, 64, 1, stride = 2)
    self.bd1 = nn.BatchNorm2d(32)
    self.bd2 = nn.BatchNorm2d(64)

    self.layers = nn.ModuleList()
    self.bn_layers = nn.ModuleList()
    # 2n layers
    for i in range(0,2*n):
      self.layers.append(nn.Conv2d(16, 16, 3, padding=1))
      self.bn_layers.append(nn.BatchNorm2d(16))
 
    # 2n layers
    self.layers.append(nn.Conv2d(16, 32, 3, stride = 2, padding=1))
    self.bn_layers.append(nn.BatchNorm2d(32))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(32, 32, 3, padding=1))
      self.bn_layers.append(nn.BatchNorm2d(32))

    # 2n layers
    self.layers.append(nn.Conv2d(32, 64, 3, stride = 2, padding=1))
    self.bn_layers.append(nn.BatchNorm2d(64))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(64, 64, 3, padding=1))
      self.bn_layers.append(nn.BatchNorm2d(64))

    # last layer
    self.avg_pool = nn.AvgPool2d(8)
    
    #Output layer
    self.linear = nn.Linear(64,r)

  def forward(self, x):
    
    x = F.relu(self.b1(self.conv1(x)))
    
    prev = torch.zeros(1,1)

    for i in range (0,6*self.n):
      
      if (i%2 == 0):
        prev = x
        x = F.relu(self.bn_layers[i](self.layers[i](x)))
      elif (i == 2*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd1(self.downsample1(prev))))
      elif (i == 4*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd2(self.downsample2(prev))))
      else:
        x = F.relu(self.bn_layers[i](self.layers[i](x) + prev))

    x = self.avg_pool(x)
    x = x.view(-1,64)
    x = self.linear(x)

    return x


# No norm 

class no_norm(nn.Module):

  def __init__(self,n,r):

    super(no_norm, self).__init__()

    self.n = n
    self.r = r

    # 1st layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)

    self.downsample1 = nn.Conv2d(16, 32, 1, stride = 2)
    self.downsample2 = nn.Conv2d(32, 64, 1, stride = 2)

    self.layers = nn.ModuleList()
    # 2n layers
    for i in range(0,2*n):
      self.layers.append(nn.Conv2d(16, 16, 3, padding=1))
 
    # 2n layers
    self.layers.append(nn.Conv2d(16, 32, 3, stride = 2, padding=1))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(32, 32, 3, padding=1))

    # 2n layers
    self.layers.append(nn.Conv2d(32, 64, 3, stride = 2, padding=1))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(64, 64, 3, padding=1))

    # last layer
    self.avg_pool = nn.AvgPool2d(8)
    
    #Output layer
    self.linear = nn.Linear(64,r)

  def forward(self, x):
    
    x = F.relu(self.conv1(x))
    
    prev = torch.zeros(1,1)

    for i in range (0,6*self.n):
      
      if (i%2 == 0):
        prev = x
        x = F.relu(self.layers[i](x))
      elif (i == 2*self.n + 1):
        x = F.relu(self.layers[i](x) + self.downsample1(prev))
      elif (i == 4*self.n + 1):
        x = F.relu(self.layers[i](x) + self.downsample2(prev))
      else:
        x = F.relu(self.layers[i](x) + prev)

    x = self.avg_pool(x)
    x = x.view(-1,64)
    x = self.linear(x)

    return x

##### Batch Norm 

class bnorm(nn.Module):

  def __init__(self, h, w, d, decay = 0.1, epsilon = 1e-5):

    super(bnorm, self).__init__()

    self.gamma = nn.Parameter(torch.ones([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.beta = nn.Parameter(torch.zeros([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.epsilon = epsilon
    self.running_mean = 0
    self.running_cov = 0
    self.decay = decay

  def forward(self, x):

    if (self.training):
      mean = torch.reshape(torch.mean(x, (0,2,3)),(1, x.shape[1], 1, 1))
      self.running_mean = (1-self.decay)*(self.running_mean) + self.decay*(mean)
      cov = torch.reshape(torch.mean(torch.square(x - mean), (0,2,3)),(1, x.shape[1], 1, 1)) + self.epsilon
      self.running_cov = (1-self.decay)*(self.running_cov) + self.decay*(cov)
      x_n  = (x - mean) / torch.sqrt(cov)
      x_s  = (x_n * self.gamma) + self.beta
      return x_s

    else:
      x_n  = (x - self.running_mean) / torch.sqrt(self.running_cov)
      x_s  = (x_n * self.gamma) + self.beta
      return x_s

class batch_norm(nn.Module):

  def __init__(self,n,r=10):

    super(batch_norm, self).__init__()

    self.n = n
    self.r = r

    # 1st layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.b1 = bnorm(32, 32, 16)

    self.downsample1 = nn.Conv2d(16, 32, 1, stride = 2)
    self.downsample2 = nn.Conv2d(32, 64, 1, stride = 2)
    self.bd1 = bnorm(16, 16 ,32)
    self.bd2 = bnorm(8, 8, 64)

    self.layers = nn.ModuleList()
    self.bn_layers = nn.ModuleList()
    # 2n layers
    for i in range(0,2*n):
      self.layers.append(nn.Conv2d(16, 16, 3, padding=1))
      self.bn_layers.append(bnorm(32, 32, 16))
 
    # 2n layers
    self.layers.append(nn.Conv2d(16, 32, 3, stride = 2, padding=1))
    self.bn_layers.append(bnorm(16, 16, 32))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(32, 32, 3, padding=1))
      self.bn_layers.append(bnorm(16, 16, 32))

    # 2n layers
    self.layers.append(nn.Conv2d(32, 64, 3, stride = 2, padding=1))
    self.bn_layers.append(bnorm(8, 8, 64))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(64, 64, 3, padding=1))
      self.bn_layers.append(bnorm(8, 8, 64))

    # last layer
    self.avg_pool = nn.AvgPool2d(8)
    
    #Output layer
    self.linear = nn.Linear(64,r)

  def forward(self, x):
    
    x = F.relu(self.b1(self.conv1(x)))
    prev = torch.zeros((1,1))

    for i in range (0,6*self.n):
      
      if (i%2 == 0):
        prev = x
        x = F.relu(self.bn_layers[i](self.layers[i](x)))
      elif (i == 2*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd1(self.downsample1(prev))))
      elif (i == 4*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd2(self.downsample2(prev))))
      else:
        x = F.relu(self.bn_layers[i](self.layers[i](x) + prev))

    x = self.avg_pool(x)
    x = x.view(-1,64)
    x = self.linear(x)

    return x

### Layer NORM ###################

class lnorm(nn.Module):

  def __init__(self, h, w, d, decay = 0.1, epsilon = 1e-5):

    super(lnorm, self).__init__()

    self.gamma = nn.Parameter(torch.ones([1, d, h, w], dtype=torch.float), requires_grad = True)
    self.beta = nn.Parameter(torch.zeros([1, d, h, w], dtype=torch.float), requires_grad = True)
    self.epsilon = epsilon

  def forward(self, x):
    
    b_size = x.shape[0]
    mean = torch.reshape(torch.mean(x, (1,2,3)), (b_size,1,1,1))
    cov = torch.reshape(torch.mean(torch.square(x - mean), (1,2,3)), (b_size,1,1,1)) + self.epsilon 
    x_n  = (x - mean) / torch.sqrt(cov)
    x_s  = (x_n * self.gamma) + self.beta
    return x_s


class layer_norm(nn.Module):

  def __init__(self,n,r=10):

    super(layer_norm, self).__init__()

    self.n = n
    self.r = r

    # 1st layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.b1 = lnorm(32, 32, 16)

    self.downsample1 = nn.Conv2d(16, 32, 1, stride = 2)
    self.downsample2 = nn.Conv2d(32, 64, 1, stride = 2)
    self.bd1 = lnorm(16, 16 ,32)
    self.bd2 = lnorm(8, 8, 64)

    self.layers = nn.ModuleList()
    self.bn_layers = nn.ModuleList()
    # 2n layers
    for i in range(0,2*n):
      self.layers.append(nn.Conv2d(16, 16, 3, padding=1))
      self.bn_layers.append(lnorm(32, 32, 16))
 
    # 2n layers
    self.layers.append(nn.Conv2d(16, 32, 3, stride = 2, padding=1))
    self.bn_layers.append(lnorm(16, 16, 32))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(32, 32, 3, padding=1))
      self.bn_layers.append(lnorm(16, 16, 32))

    # 2n layers
    self.layers.append(nn.Conv2d(32, 64, 3, stride = 2, padding=1))
    self.bn_layers.append(lnorm(8, 8, 64))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(64, 64, 3, padding=1))
      self.bn_layers.append(lnorm(8, 8, 64))

    # last layer
    self.avg_pool = nn.AvgPool2d(8)
    
    #Output layer
    self.linear = nn.Linear(64,r)

  def forward(self, x):
    
    x = F.relu(self.b1(self.conv1(x)))
    prev = torch.zeros((1,1))

    for i in range (0,6*self.n):
      
      if (i%2 == 0):
        prev = x
        x = F.relu(self.bn_layers[i](self.layers[i](x)))
      elif (i == 2*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd1(self.downsample1(prev))))
      elif (i == 4*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd2(self.downsample2(prev))))
      else:
        x = F.relu(self.bn_layers[i](self.layers[i](x) + prev))

    x = self.avg_pool(x)
    x = x.view(-1,64)
    x = self.linear(x)

    return x

  
### Instance Norm ################

class inorm(nn.Module):

  def __init__(self, h, w, d, decay = 0.1, epsilon = 1e-5):

    super(inorm, self).__init__()

    self.gamma = nn.Parameter(torch.ones([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.beta = nn.Parameter(torch.zeros([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.epsilon = epsilon

  def forward(self, x):
    
    b_size = x.shape[0]
    channel = x.shape[1]
    mean = torch.reshape(torch.mean(x, (2,3)), (b_size, channel,1,1))
    cov = torch.reshape(torch.mean(torch.square(x - mean), (2,3)), (b_size,channel,1,1)) + self.epsilon 
    x_n  = (x - mean) / torch.sqrt(cov)
    x_s  = (x_n * self.gamma) + self.beta
    return x_s
  

class instance_norm(nn.Module):

  def __init__(self,n,r=10):

    super(instance_norm, self).__init__()

    self.n = n
    self.r = r

    # 1st layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.b1 = inorm(32, 32, 16)

    self.downsample1 = nn.Conv2d(16, 32, 1, stride = 2)
    self.downsample2 = nn.Conv2d(32, 64, 1, stride = 2)
    self.bd1 = inorm(16, 16 ,32)
    self.bd2 = inorm(8, 8, 64)

    self.layers = nn.ModuleList()
    self.bn_layers = nn.ModuleList()
    # 2n layers
    for i in range(0,2*n):
      self.layers.append(nn.Conv2d(16, 16, 3, padding=1))
      self.bn_layers.append(inorm(32, 32, 16))
 
    # 2n layers
    self.layers.append(nn.Conv2d(16, 32, 3, stride = 2, padding=1))
    self.bn_layers.append(inorm(16, 16, 32))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(32, 32, 3, padding=1))
      self.bn_layers.append(inorm(16, 16, 32))

    # 2n layers
    self.layers.append(nn.Conv2d(32, 64, 3, stride = 2, padding=1))
    self.bn_layers.append(inorm(8, 8, 64))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(64, 64, 3, padding=1))
      self.bn_layers.append(inorm(8, 8, 64))

    # last layer
    self.avg_pool = nn.AvgPool2d(8)
    
    #Output layer
    self.linear = nn.Linear(64,r)

  def forward(self, x):
    
    x = F.relu(self.b1(self.conv1(x)))
    prev = torch.zeros((1,1))

    for i in range (0,6*self.n):
      
      if (i%2 == 0):
        prev = x
        x = F.relu(self.bn_layers[i](self.layers[i](x)))
      elif (i == 2*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd1(self.downsample1(prev))))
      elif (i == 4*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd2(self.downsample2(prev))))
      else:
        x = F.relu(self.bn_layers[i](self.layers[i](x) + prev))

    x = self.avg_pool(x)
    x = x.view(-1,64)
    x = self.linear(x)

    return x

### Group Norm ################

class gnorm(nn.Module):

  def __init__(self, h, w, d, epsilon = 1e-5, group_size = 16):

    super(gnorm, self).__init__()

    self.gamma = nn.Parameter(torch.ones([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.beta = nn.Parameter(torch.zeros([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.epsilon = epsilon
    self.group_size = group_size

  def forward(self, x):
    
    b_size = x.shape[0]
    channel = x.shape[1]
    k = int(channel/self.group_size)

    x = torch.reshape(x, (b_size, self.group_size, k, x.shape[2], x.shape[3] ))

    mean = torch.reshape(torch.mean(x, (2,3,4)), (b_size, self.group_size, 1, 1, 1))
    cov = torch.reshape(torch.mean(torch.square(x - mean), (2, 3, 4)), (b_size,self.group_size, 1, 1, 1)) + self.epsilon 
    x_n  = (x - mean) / torch.sqrt(cov)
    x_n = torch.reshape(x_n, (b_size, channel, x.shape[3], x.shape[4]))
    x_s  = (x_n * self.gamma) + self.beta
    return x_s


class group_norm(nn.Module):

  def __init__(self,n,r=10):

    super(group_norm, self).__init__()

    self.n = n
    self.r = r

    # 1st layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.b1 = gnorm(32, 32, 16)

    self.downsample1 = nn.Conv2d(16, 32, 1, stride = 2)
    self.downsample2 = nn.Conv2d(32, 64, 1, stride = 2)
    self.bd1 = gnorm(16, 16 ,32)
    self.bd2 = gnorm(8, 8, 64)

    self.layers = nn.ModuleList()
    self.bn_layers = nn.ModuleList()
    # 2n layers
    for i in range(0,2*n):
      self.layers.append(nn.Conv2d(16, 16, 3, padding=1))
      self.bn_layers.append(gnorm(32, 32, 16))
 
    # 2n layers
    self.layers.append(nn.Conv2d(16, 32, 3, stride = 2, padding=1))
    self.bn_layers.append(gnorm(16, 16, 32))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(32, 32, 3, padding=1))
      self.bn_layers.append(gnorm(16, 16, 32))

    # 2n layers
    self.layers.append(nn.Conv2d(32, 64, 3, stride = 2, padding=1))
    self.bn_layers.append(gnorm(8, 8, 64))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(64, 64, 3, padding=1))
      self.bn_layers.append(gnorm(8, 8, 64))

    # last layer
    self.avg_pool = nn.AvgPool2d(8)
    
    #Output layer
    self.linear = nn.Linear(64,r)

  def forward(self, x):
    
    x = F.relu(self.b1(self.conv1(x)))
    prev = torch.zeros((1,1))

    for i in range (0,6*self.n):
      
      if (i%2 == 0):
        prev = x
        x = F.relu(self.bn_layers[i](self.layers[i](x)))
      elif (i == 2*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd1(self.downsample1(prev))))
      elif (i == 4*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd2(self.downsample2(prev))))
      else:
        x = F.relu(self.bn_layers[i](self.layers[i](x) + prev))

    x = self.avg_pool(x)
    x = x.view(-1,64)
    x = self.linear(x)

    return x



#### Batch Instance Norm ############

class binorm(nn.Module):

  def __init__(self, h, w, d, decay = 0.1, epsilon = 1e-5):

    super(binorm, self).__init__()

    self.gamma = nn.Parameter(torch.ones([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.beta = nn.Parameter(torch.zeros([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.rho = nn.Parameter(torch.zeros([1, d, 1, 1], dtype=torch.float), requires_grad = True)
    self.epsilon = epsilon
    self.running_mean = 0
    self.running_cov = 0
    self.decay = decay

  def forward(self, x):

    b_size = x.shape[0]
    channel = x.shape[1]

    if (self.training):
      mean = torch.reshape(torch.mean(x, (0,2,3)),(1, x.shape[1], 1, 1))
      self.running_mean = (1-self.decay)*(self.running_mean) + self.decay*(mean)
      cov = torch.reshape(torch.mean(torch.square(x - mean), (0,2,3)),(1, x.shape[1], 1, 1)) + self.epsilon
      self.running_cov = (1-self.decay)*(self.running_cov) + self.decay*(cov)
      x_n1  = (x - mean) / torch.sqrt(cov)
      
      mean2 = torch.reshape(torch.mean(x, (2,3)), (b_size, channel,1,1))
      cov2 = torch.reshape(torch.mean(torch.square(x - mean2), (2,3)), (b_size,channel,1,1)) + self.epsilon 
      x_n2  = (x - mean2) / torch.sqrt(cov2)

      x_nc = x_n1*self.rho + x_n2*(1 - self.rho)
      x_s  = (x_nc * self.gamma) + self.beta
      return x_s


    else:
      x_n1  = (x - self.running_mean) / torch.sqrt(self.running_cov)
      
      mean2 = torch.reshape(torch.mean(x, (2,3)), (b_size, channel,1,1))
      cov2 = torch.reshape(torch.mean(torch.square(x - mean2), (2,3)), (b_size,channel,1,1)) + self.epsilon 
      x_n2  = (x - mean2) / torch.sqrt(cov2)

      x_nc = x_n1*self.rho + x_n2*(1 - self.rho)
      x_s  = (x_nc * self.gamma) + self.beta
      return x_s


class binstance_norm(nn.Module):

  def __init__(self,n,r=10):

    super(binstance_norm, self).__init__()

    self.n = n
    self.r = r

    # 1st layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.b1 = binorm(32, 32, 16)

    self.downsample1 = nn.Conv2d(16, 32, 1, stride = 2)
    self.downsample2 = nn.Conv2d(32, 64, 1, stride = 2)
    self.bd1 = binorm(16, 16 ,32)
    self.bd2 = binorm(8, 8, 64)

    self.layers = nn.ModuleList()
    self.bn_layers = nn.ModuleList()
    # 2n layers
    for i in range(0,2*n):
      self.layers.append(nn.Conv2d(16, 16, 3, padding=1))
      self.bn_layers.append(binorm(32, 32, 16))
 
    # 2n layers
    self.layers.append(nn.Conv2d(16, 32, 3, stride = 2, padding=1))
    self.bn_layers.append(binorm(16, 16, 32))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(32, 32, 3, padding=1))
      self.bn_layers.append(binorm(16, 16, 32))

    # 2n layers
    self.layers.append(nn.Conv2d(32, 64, 3, stride = 2, padding=1))
    self.bn_layers.append(binorm(8, 8, 64))
    for i in range(0,2*n-1):
      self.layers.append(nn.Conv2d(64, 64, 3, padding=1))
      self.bn_layers.append(binorm(8, 8, 64))

    # last layer
    self.avg_pool = nn.AvgPool2d(8)
    
    #Output layer
    self.linear = nn.Linear(64,r)

  def forward(self, x):
    
    x = F.relu(self.b1(self.conv1(x)))
    prev = torch.zeros((1,1))

    for i in range (0,6*self.n):
      
      if (i%2 == 0):
        prev = x
        x = F.relu(self.bn_layers[i](self.layers[i](x)))
      elif (i == 2*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd1(self.downsample1(prev))))
      elif (i == 4*self.n + 1):
        x = F.relu(self.bn_layers[i](self.layers[i](x) + self.bd2(self.downsample2(prev))))
      else:
        x = F.relu(self.bn_layers[i](self.layers[i](x) + prev))

    x = self.avg_pool(x)
    x = x.view(-1,64)
    x = self.linear(x)

    return x

#########################
def test (model, X, output_path):

  with torch.no_grad():
  
    for i in range (0,X.shape[0],1000):
      
      pred = model.forward(X[i:min(X.shape[0],i+1000),:,:,:])
      r = pred.size(0)
      pred = pred.data
      
      max_val, max_ind = torch.max(pred, 1)
      max_ind = max_ind.tolist()
      with open(output_path, 'a') as file:
        for label in max_ind:
          file.write("%i\n" % label)
      

########################
################ Parse Command line arguments ##############

normalisation = {
                 "bn":      0,
                 "in":      1, 
                 "bin":     2,
                 "ln":      3,
                 "gn":      4,
                 "nn":      5,
                 "torch_bn":6

}
model_to_use = [batch_norm, instance_norm, binstance_norm, layer_norm,group_norm, no_norm, torch_bn]

norm_used = -1
test_data_path = ""
output_path = ""
numb_rcell = 0 
model_path = ""

argumentList = sys.argv[1:]
 
# Options
options = "n:"
 
# Long options
long_options = ["model_file=","n=", "normalization=","output_file=", "test_data_file="]
 
try:
    # Parsing argument
    arguments, values = getopt.getopt(argumentList, options, long_options)
     
    # checking each argument
    for currentArgument, currentValue in arguments:

      if currentArgument in ("--model_file"):
        model_path = currentValue 
          
      elif currentArgument in ("--test_data_file"):
        test_data_path = currentValue 

      elif currentArgument in ("--output_file"):
        output_path = currentValue

      elif currentArgument in ("-n", "--n"):
        numb_rcell = currentValue

      elif currentArgument in ("--normalization"):
        norm_used = normalisation[currentValue]
             
except getopt.error as err:
    # output error, and return with an error code
    print (str(err))

print(model_path, test_data_path, output_path, numb_rcell, norm_used)
if __name__ == "__main__":

  X_test_t = get_data(test_data_path)
  X_test_t = X_test_t.to(device)
  
  neural_net = torch.load(model_path)
  neural_net.cuda()
  neural_net.eval()

  test(neural_net , X_test_t, output_path)