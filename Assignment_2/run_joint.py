# -*- coding: utf-8 -*-
"""Combined.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TgsQmro5PdLkY5PCVJ5MTM2PUNymWQNg
"""

import sys
import os

# <path_to_train> <path_to_test_query> <path_to_output_file> <path_to_sample_images.npy> gen1k_joint.npy target1k_joint.npy output_joint.csv

path_to_train  = str(sys.argv[1])
path_to_test_query   = str(sys.argv[2])
path_to_output_file  = str(sys.argv[3])
path_to_sample_images  = str(sys.argv[4])
path_to_gen1k_joint  = str(sys.argv[5])
path_to_target1k_joint   = str(sys.argv[6])
path_to_output_joint  = str(sys.argv[7])

import   numpy as np
from     sklearn.cluster import KMeans
import   matplotlib.pyplot as plt
from     PIL import Image
import   cv2
import   torch
import   torch.nn as nn
import   torch.nn.functional as F
import   torch.optim as optim
from     torch.utils.data.sampler import SubsetRandomSampler
from     torch.utils.data import Dataset, DataLoader, TensorDataset
from     torchvision import datasets, transforms
from     sklearn.neighbors import KNeighborsClassifier
import   copy
import   csv

query = np.load('./query.npy')
target = np.load('./target.npy')

query = query.astype(float)/255
target = target.astype(float)/255

query = query.reshape((-1, 64, 1, 28, 28))
target = target.reshape((-1, 64, 1, 28, 28))

class MyDataset (Dataset):

  def __init__(self, X, Y, R, C):
    self.X = X
    self.Y = Y
    self.R = R
    self.C = C

  def __len__(self):
    return (self.X).shape[0]

  def __getitem__(self, idx):
    return torch.from_numpy(self.X[idx]), torch.from_numpy(self.Y[idx]), torch.from_numpy(self.R[idx]), torch.from_numpy(self.C[idx])

row =  np.array([0,1,2,3,4,5,6,7])
rows = np.tile(np.tile(row, 8), (query.shape[0],1))

col =  np.array([0,1,2,3,4,5,6,7])
cols = np.tile(np.repeat(col , 8), (query.shape[0], 1))

b_size = 16
train_dataset = MyDataset(query[:8000,:],target[:8000,:],rows[:8000, :], cols[:8000, :])
val_dataset = MyDataset(query[8000:,:],target[8000:,:],rows[8000:, :], cols[8000:, :])
train_loader = DataLoader(train_dataset, batch_size = b_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size = b_size, shuffle=False)
train_loader_pred = DataLoader(train_dataset, batch_size = b_size, shuffle=False)

"""# **Classifier**"""

class classifier(nn.Module):
 
  def __init__(self):
 
    super(classifier, self).__init__()
    
    self.conv1 = nn.Conv2d(1, 32, 3)
    self.pool1 = nn.MaxPool2d(2, stride= 2)
 
    self.conv2 = nn.Conv2d(32, 64, 3)
    self.pool2 = nn.MaxPool2d(2, stride= 2)
 
    self.linear1 = nn.Linear(25*64, 128)
    self.linear2 = nn.Linear(128, 9)
 
  def forward(self, x):
 
    x = F.relu(self.conv1(x))
    x = self.pool1(x)
    x = F.relu(self.conv2(x))
    x = self.pool2(x)
 
    x = x.view(-1, 25*64)
    x = F.relu(self.linear1(x))
    x = self.linear2(x)
    return torch.softmax(x, dim = 1)

"""# **RRN**"""

class MLP(nn.Module):

  def __init__(self, size):

    super(MLP, self).__init__()

    self.linear1 = nn.Linear(size, 96)
    self.linear2 = nn.Linear(96, 96)
    # self.linear3 = nn.Linear(96, 96)
    self.linear4 = nn.Linear(96, 96)

    # self.dropZ = nn.Dropout()
    
  def forward(self, x):

    x = F.relu(self.linear1(x))
    x = F.relu(self.linear2(x))
    # x = F.relu(self.linear3(x))
    x = self.linear4(x)
    
    return x

class embedx(nn.Module):

  def __init__(self):

    super(embedx, self).__init__()

    self.emb_input  = nn.Embedding(9, 16)
    self.emb_row    = nn.Embedding(8, 16)
    self.emb_col    = nn.Embedding(8, 16)
    # self.MLP = MLP(48)
    self.MLP = MLP(16)
    
  def forward(self, x, r, c):

    x = self.emb_input(x)
    r = self.emb_row(r)
    c = self.emb_col(c)

    # conc = torch.cat((x, r, c), dim = 2 )

    # out = self.MLP(conc)
    out = self.MLP(x)
    return out

class msg_fn(nn.Module):

  def __init__(self):

    super(msg_fn, self).__init__()
   
    self.MLP = MLP(96*2)
    
  def forward(self, hi, hj):

    conc = torch.cat((hi, hj), dim = 2) # unidirectional message from node i to node j
    out = self.MLP(conc)
    return out  

class node_fn(nn.Module): # Effective LSTM Cell

  def __init__(self):

    super(node_fn, self).__init__()

    self.LSTMG = nn.LSTMCell(96,96)
    self.MLP = MLP(96*2)
    
  def forward(self, x, mt, st_1, ht_1):

    conc = torch.cat((x, mt), dim = 2)
    out = self.MLP(conc)
    
    out = torch.reshape(out, (-1, out.shape[2]))
    st_1 = torch.reshape(st_1, (-1, st_1.shape[2]))
    ht_1 = torch.reshape(ht_1, (-1, ht_1.shape[2]))

    ht, st = self.LSTMG(out, (ht_1, st_1))

    ht = torch.reshape(ht, x.shape)
    st = torch.reshape(st, x.shape)
    return ht, st 

class out_layer(nn.Module):

  def __init__(self):

    super(out_layer, self).__init__()
    self.linear = nn.Linear(96, 8)
  
  def forward(self, x):

    x = self.linear(x)
    return torch.softmax(x, dim=2)
########################################
  # hx size = (batch_size, 64, 96)
  # sx size = (batch_size, 64, 96)
  # x  size = (batch_size, 64, 96)
  # msg_matrix = (batch_size, 64, 64, 96)

class RRN(nn.Module):

  def __init__(self):

    super(RRN, self).__init__()
    
    self.emb    = embedx() # embedding of x, r, c
    self.msf    = msg_fn() # Message from cell i to cell j
    self.nf     = node_fn() # LSTM cell
    self.ol     = out_layer() # maps hidden layer to output layer
    self.b_size = 16


    ## create mask ######
    self.mask = torch.zeros(64,self.b_size, 64, 96).cuda() # (64, batch size, 64, feature size)

    for i in range(64):
      indices = []

      column = int(i/8) 
      indices = indices + [ (8*column + j) for j in range(8) ]
      indices = indices + [ ( i + 8*j) for j in range(-column, 8 - column) ]

      even = (i%2 == 0)
      left = (column < 4)

      if (even and left):
        start = i - 8*column
        indices = indices + [ (start + 8*j) for j in range(4)]
        indices = indices + [ (start + 8*j + 1) for j in range(4)]

      elif ((not even) and left):
        start = i - 8*column
        indices = indices + [ (start + 8*j) for j in range(4)]
        indices = indices + [ (start + 8*j - 1) for j in range(4)]
        

      elif ((not even) and (not left)):
        start = i - 8*(column -4)
        indices = indices + [ (start + 8*j) for j in range(4)]
        indices = indices + [ (start + 8*j - 1) for j in range(4)]        

      elif (even and (not left)):
        start = i - 8*(column -4)
        indices = indices + [ (start + 8*j) for j in range(4)]
        indices = indices + [ (start + 8*j + 1) for j in range(4)]
      
      
      indices = list(set(indices))

      for ind in indices:
        self.mask[i,:, ind,:] = 1

    
  def forward(self, x, r, c, hx, sx, time_step):
    
    x = self.emb(x, r, c)

    if (time_step ==0):
      hx = x
      
    h1 = torch.tile(hx, (1,64, 1))
    h2 = torch.repeat_interleave(hx, 64, dim=1)

    msg_matrix = self.msf(h1, h2)
    sub_matrix = torch.tensor_split(msg_matrix, 64, dim = 1)

    ls = []
    for count, sm in enumerate(sub_matrix):
      ls.append(torch.reshape(torch.sum(sm * self.mask[count, :,:,:],1 ), (sm.shape[0], 1, sm.shape[2])))
    
    msg_matrix = torch.cat(ls, dim=1)

    ht, st = self.nf(x, msg_matrix ,sx, hx)

    pred = self.ol(ht)
    return ht, st , pred

"""# **GAN**"""

class generator(nn.Module):

  def __init__(self):

    super(generator, self).__init__()

    self.linear1_z = nn.Linear(100, 200)
    self.linear1_y = nn.Linear(9,1000)

    self.dropZ = nn.Dropout()
    self.dropY = nn.Dropout()

    self.linear2_comb = nn.Linear(1200,1200)
    self.dropZY = nn.Dropout()

    self.linear3 = nn.Linear(1200,784)

  def forward(self, z, y):

    z = F.leaky_relu( self.dropZ(self.linear1_z(z)), negative_slope=0.2)
    y = F.leaky_relu( self.dropY(self.linear1_y(y)), negative_slope=0.2)

    zy = torch.cat((z, y), 1 )
    zy = F.leaky_relu(self.dropZY(self.linear2_comb(zy)), negative_slope=0.2)

    z = self.linear3(zy)
    z = torch.sigmoid(z)

    return z

class discriminator(nn.Module):

  def __init__(self):

    super(discriminator, self).__init__()

    self.linear1_1_x = nn.Linear(784,240)
    self.linear1_2_x = nn.Linear(784,240)
    self.linear1_3_x = nn.Linear(784,240)
    self.linear1_4_x = nn.Linear(784,240)
    self.linear1_5_x = nn.Linear(784,240)

    self.dropX = nn.Dropout()

    self.linear1_1_y = nn.Linear(9,50)
    self.linear1_2_y = nn.Linear(9,50)
    self.linear1_3_y = nn.Linear(9,50)
    self.linear1_4_y = nn.Linear(9,50)
    self.linear1_5_y = nn.Linear(9,50)

    self.dropY = nn.Dropout()

    self.linear2_1 = nn.Linear(290,240)
    self.linear2_2 = nn.Linear(290,240)
    self.linear2_3 = nn.Linear(290,240)
    self.linear2_4 = nn.Linear(290,240)

    self.dropXY = nn.Dropout()

    self.linear3 = nn.Linear(240, 1)

  def forward(self, x, y):

    x11 = self.linear1_1_x(x)
    x12 = self.linear1_2_x(x)
    x13 = self.linear1_3_x(x)
    x14 = self.linear1_4_x(x)
    x15 = self.linear1_5_x(x)
    x = torch.maximum(x11, torch.maximum(x12, torch.maximum(x13, torch.maximum(x14, x15))))
    x = self.dropX(x)
    x = F.relu(x)

    y11 = self.linear1_1_y(y)
    y12 = self.linear1_2_y(y)
    y13 = self.linear1_3_y(y)
    y14 = self.linear1_4_y(y)
    y15 = self.linear1_5_y(y)
    y = torch.maximum(y11, torch.maximum(y12, torch.maximum(y13, torch.maximum(y14, y15))))
    y = self.dropY(y)
    y = F.relu(y)

    xy = torch.cat((x,y), 1)

    x21 = self.linear2_1(xy)
    x22 = self.linear2_2(xy)
    x23 = self.linear2_3(xy)
    x24 = self.linear2_4(xy)

    x = torch.maximum(x21, torch.maximum(x22, torch.maximum(x23, x24)))
    x = self.dropXY(x)
    x = F.relu(x)
    y = x

    x = self.linear3(x)
    x = torch.sigmoid(x)
    return x, y

"""# **Training**"""

epochs = 20

n_steps = 30

cnn = classifier().cuda()
rrn = RRN().cuda()
g = generator().cuda()
d = discriminator().cuda()

optm_cnn = optim.Adam(cnn.parameters(), lr = 1e-2, weight_decay= 0.0001)
optm_rrn = optim.Adam(rrn.parameters(), lr = 2e-4, weight_decay= 0.0001)
optm_g = optim.RMSprop(g.parameters(), lr = 1e-4, momentum = 0.5, weight_decay= 0.0001)
optm_d = optim.RMSprop(d.parameters(), lr = 2e-4, momentum = 0.5, weight_decay= 0.0001)

count = 0
k = 1
mode = True
feature_reg = 0.3

for e in range(0, epochs):

  print("Epoch : "+str(e+1))

  loss_t = 0
  loss_d = 0
  loss_g = 0

  for data in train_loader:

    optm_cnn.zero_grad()
    optm_rrn.zero_grad()

    x = data[0].cuda().float() # batch_size x 64 x 1 x 28 x 28 query
    y = data[1].cuda().float() # batch_size x 64 x 1 x 28 x 28 target
    R = data[2].cuda()
    C = data[3].cuda()

# ---------- CNN -------------------

    x_pred_prob = cnn(x.reshape(-1, 1, 28, 28)).reshape(-1, 64, 9) # batch_size x 64 x 9 query
    y_pred_prob = cnn(y.reshape(-1, 1, 28, 28)).reshape(-1, 64, 9) # batch_size x 64 x 9 target

    x_pred_labels = torch.argmax(x_pred_prob, dim = 2) # batch_size x 64 query
    y_pred_labels = torch.argmax(y_pred_prob[:,:,1:], dim = 2) # batch_size x 64 target

# ---------- RRN -------------------

    hx = torch.zeros(x_pred_labels.shape[0], x_pred_labels.shape[1], 96).cuda()
    sx = torch.zeros(x_pred_labels.shape[0], x_pred_labels.shape[1], 96).cuda()

    h = []
    s = [] 
    p = [] # predictions over n_steps
    h.append(hx)
    s.append(sx)

    for step in range(n_steps):
      ht, st, pred = rrn.forward(x_pred_labels, R, C, h[-1], s[-1], step) # time_steps * batch_size * 64 * 96 
      h.append(ht)
      s.append(st)
      p.append(pred)

    ll = 0

    for step in range(n_steps):
      ll += -(torch.log(torch.gather(p[step], 2, (y_pred_labels - 1).unsqueeze(-1)))).sum()

    # ll.backward()
    loss_t += (ll.item()*b_size)/n_steps

# ---------- GAN -------------------

    count += 1

    x = (data[0].cuda()).float().view(-1,784).detach()
    z = (torch.randn(x.shape[0], 100).cuda()).float().detach()
    y = x_pred_prob.view(-1, 9).detach()

    D_real, f_real = d(x, y)
    D_gen , f_gen  = d(g(z, y), y)

    # loss = -(torch.log(D_real + 1e-8) + torch.log(1 - D_gen+ 1e-8)).mean()

    if (mode == True):
      loss = -(torch.log(D_real + 1e-8)).mean()
      mode = False 
    else: 
      loss = -(torch.log(1 - D_gen+ 1e-8)).mean()
      mode = True 

    loss.backward()

    loss_d += (loss.item())*b_size

    optm_d.step()

    if (count == k):

      z = (torch.randn(x.shape[0], 100).cuda()).float()

      count = 0
      optm_g.zero_grad()

      D_real, f_real = d(x, y)
      D_gen , f_gen  = d(g(z, y), y)

      # loss = torch.log(1 - D_gen + 1e-8).mean() + feature_reg * (torch.linalg.norm(f_real - f_gen, dim=1).mean())
      loss = - torch.log(D_gen + 1e-8).mean() + feature_reg * (torch.linalg.norm(f_real - f_gen, dim=1).mean())

      loss.backward()
      loss_g += (loss.item())*b_size

      optm_g.step()

    ll.backward()
    optm_cnn.step()
    optm_rrn.step()

    torch.cuda.empty_cache()
  print("Training")
  print("RRN Loss: " + str(loss_t/8000) + " Generator Loss: " + str(loss_g/8000) + " Discriminator Loss: " + str(loss_d/8000))

#------------------------------Validation---------------------------------------

  loss_t_val = 0
  loss_d_val = 0
  loss_g_val = 0

  for data in val_loader:

    with torch.no_grad():

      x = data[0].cuda().float() # batch_size x 64 x 1 x 28 x 28 query
      y = data[1].cuda().float() # batch_size x 64 x 1 x 28 x 28 target
      R = data[2].cuda()
      C = data[3].cuda()

#--------------------------------- CNN ----------------------------------------

      x_pred_prob = cnn(x.reshape(-1, 1, 28, 28)).reshape(-1, 64, 9) # batch_size x 64 x 9 query
      y_pred_prob = cnn(y.reshape(-1, 1, 28, 28)).reshape(-1, 64, 9) # batch_size x 64 x 9 target

      x_pred_labels = torch.argmax(x_pred_prob, dim = 2) # batch_size x 64 query
      y_pred_labels = torch.argmax(y_pred_prob[:,:,1:], dim = 2) # batch_size x 64 target

#--------------------------------- RRN ----------------------------------------

      hx = torch.zeros(x_pred_labels.shape[0], x_pred_labels.shape[1], 96).cuda()
      sx = torch.zeros(x_pred_labels.shape[0], x_pred_labels.shape[1], 96).cuda()
      pred = None

      for step in range(n_steps):
        hx, sx, pred = rrn.forward(x_pred_labels, R, C, hx, sx, step) # time_steps * batch_size * 64 * 96 

      ll = -(torch.log(torch.gather(p[step], 2, (y_pred_labels - 1).unsqueeze(-1)))).sum()
      loss_t_val += (ll.item()*b_size)

# ---------------------------------- GAN ---------------------------------------

      x = (data[0].cuda()).float().view(-1,784).detach()
      z = (torch.randn(x.shape[0], 100).cuda()).float().detach()
      y = x_pred_prob.view(-1, 9).detach()

      D_real, f_real = d(x, y)
      D_gen , f_gen  = d(g(z, y), y)

      if (mode == True):
        loss = -(torch.log(D_real + 1e-8)).mean()
        mode = False 
      else: 
        loss = -(torch.log(1 - D_gen+ 1e-8)).mean()
        mode = True 

      loss_d_val += (loss.item())*b_size

      z = (torch.randn(x.shape[0], 100).cuda()).float()
      D_real, f_real = d(x, y)
      D_gen , f_gen  = d(g(z, y), y)

      loss = - torch.log(D_gen + 1e-8).mean() + feature_reg * (torch.linalg.norm(f_real - f_gen, dim=1).mean())
      loss_g_val += (loss.item())*b_size

      torch.cuda.empty_cache()
  print("Validation")
  print("RRN Loss: " + str(loss_t_val/2000) + " Generator Loss: " + str(loss_g_val/2000) + " Discriminator Loss: " + str(loss_d_val/2000))

g.eval()

y = [[0, 0, 0, 0, 0, 0, 0, 0, 0]]

generated_labelled_data = np.zeros((9000,784))
labels = np.zeros((9000,1))

with torch.no_grad():    
  for i in range(0, 9):
    for j in range(0, 1000):

      x = copy.deepcopy(y)
      x[0][i] = 1
      
      x = (torch.tensor(x).float()).cuda()
      z = (torch.randn(1, 100).cuda()).float()
      output = ((g(z,x)[0]).cpu()).detach().numpy()
      generated_labelled_data[i*1000 + j] = output
      labels[i*1000 + j] = i

np.save(path_to_gen1k_joint, generated_labelled_data)
np.save(path_to_target1k_joint, labels)

class MyDataset3 (Dataset):

  def __init__(self, X,R, C):
    self.X = X
    # self.Y = Y
    self.R = R
    self.C = C

  def __len__(self):
    return (self.X).shape[0]

  def __getitem__(self, idx):
    return torch.from_numpy(self.X[idx]), torch.from_numpy(self.R[idx]), torch.from_numpy(self.C[idx])

train_img_path  = path_to_test_query
num_images      = len(os.listdir(train_img_path))

digit_collection = []

for i in range(num_images): 
  img = Image.open( path_to_test_query + '/' + str(i)+'.png')
  img = np.asarray(img) 
  h_tiles = np.hsplit(img, 8)
  for tile in h_tiles:
    digit_collection = digit_collection + np.vsplit(tile, 8)
X_train = (np.stack(digit_collection))/255.0 #Normalise
# X_train = X_train.astype(np.int8)
np.save("./joint_test", X_train.astype(np.int8))

cl = cnn

b_size = 512
X = np.reshape(X_train, (-1, 1, 28, 28))
X_t = torch.Tensor(X)
 
p = []
 
test_data   = TensorDataset(X_t) # create your datset
test_loader = DataLoader( test_data, 
                          batch_size=b_size, 
                          shuffle=False)
 
for data in test_loader:
  input = data[0].cuda().float()
  output = cl(input)
 
  output = torch.argmax(output, dim=1)
 
  p =  p + output.tolist()
  
p = np.array(p).reshape(-1,1)

p = p.astype(np.int)
# print(p.shape)
np.save('./joint_test_labels_gan.npy',p)


input = p


num_images = int(input.shape[0]/64)

input = np.reshape(input, (-1,64))


if(input.shape[0]%16 != 0):
    temp = np.zeros((input.shape[0] + 16 - input.shape[0]%16, input.shape[1]))
    temp[:input.shape[0],:] = input 
    input= temp 

input = input.astype(np.int)

row =  np.array([0,1,2,3,4,5,6,7])
rows = np.tile(np.tile(row, 8), (input.shape[0],1))

col =  np.array([0,1,2,3,4,5,6,7])
cols = np.tile(np.repeat(col , 8), (input.shape[0], 1))

b_size = 16
test_dataset = MyDataset3(input[:,:],rows[:, :], cols[:, :])
test_loader = DataLoader(test_dataset, batch_size = b_size, shuffle=False)

def get_labels(data_loader, neural_net):

  predl = []

  with torch.no_grad():
    for data in data_loader:

        X = data[0].cuda()
        R = data[1].cuda()
        C = data[2].cuda()

        hx = torch.zeros(X.shape[0], X.shape[1], 96).cuda()
        sx = torch.zeros(X.shape[0], X.shape[1], 96).cuda()
        pred = None

        for step in range(30):
          hx, sx, pred = neural_net.forward(X, R, C, hx, sx, step)

        for i in range(0, X.shape[0]):
          pred_labels = (torch.argmax(pred[i], dim = 1)).int()
          pred_labels += 1 
          predl.append(pred_labels.view(-1))
  
  return predl

pred =  get_labels(test_loader, rrn)

with open(path_to_output_file + '/' + path_to_output_joint, 'w') as file:
  writer = csv.writer(file)
  for i in range(num_images):
    writer.writerow([ (str(i)+".png")] + pred[i].tolist())
